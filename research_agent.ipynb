{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gqcpm/scholar_stream/blob/main/research_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IvQACbeaEjPu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install langgraph langchain_core arxiv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import arxiv\n",
        "from typing import TypedDict, List, Annotated\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "cosAG5-8GlAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo4aEF-WBXEh"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Load the BASE model (The big 14B one)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\", # Changed from Qwen3-14B to Qwen3-8B\n",
        "    max_seq_length = 1024,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")\n",
        "\n",
        "# 2. Load your ADAPTERS on top (The files you just saved)\n",
        "model.load_adapter(\"/content/drive/MyDrive/ai_models/lora_adapters\")\n",
        "\n",
        "# 3. Enable Inference Speedup\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"Model loaded successfully from Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. DEFINE THE STATE ---\n",
        "# This dictionary tracks the data as it moves between agents\n",
        "class ResearchState(TypedDict):\n",
        "    task: str               # The user's original question\n",
        "    plan: List[str]         # The list of steps to research\n",
        "    content: List[str]      # The raw data gathered from ArXiv\n",
        "    draft: str              # The current written report\n",
        "    critique: str           # Feedback from the critic\n",
        "    revision_number: int    # To track iterations\n",
        "    max_revisions: int      # Limit to stop infinite loops\n",
        "\n",
        "# --- 2. HELPER: CONNECT UNSLOTH MODEL ---\n",
        "# This function wraps your loaded 'model' and 'tokenizer' to work like a chat bot\n",
        "def call_local_model(messages, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Formats messages for Qwen/Unsloth and generates a response.\n",
        "    \"\"\"\n",
        "    # Apply the specific chat template for your model (Qwen handles this well)\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Create inputs\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    # Decode and strip the prompt (so we only get the new response)\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response_text\n",
        "\n",
        "# --- 3. DEFINE THE NODES (AGENTS) ---\n",
        "\n",
        "def planner_node(state: ResearchState):\n",
        "    print(\"--- üß† PLANNER IS THINKING ---\")\n",
        "\n",
        "    # Construct the prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a Research Planner. Return a Python list of 3 short, specific search queries related to the user's task. Example format: ['query 1', 'query 2', 'query 3']. Do not explain, just return the list.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Task: {state['task']}\"}\n",
        "    ]\n",
        "\n",
        "    # Get response from your local model\n",
        "    response = call_local_model(messages)\n",
        "\n",
        "    # Simple parsing to ensure we get a list (Basic robustness)\n",
        "    # If the model chats too much, we try to extract the list part\n",
        "    try:\n",
        "        # Try to find the bracketed list in the text\n",
        "        import ast\n",
        "        start = response.find('[')\n",
        "        end = response.rfind(']') + 1\n",
        "        plan = ast.literal_eval(response[start:end])\n",
        "    except:\n",
        "        # Fallback if model fails to output strict list\n",
        "        plan = [f\"{state['task']} generic analysis\", f\"{state['task']} method comparison\"]\n",
        "\n",
        "    return {\"plan\": plan}\n",
        "\n",
        "def researcher_node(state: ResearchState):\n",
        "    print(\"--- üïµÔ∏è RESEARCHER IS SEARCHING ARXIV ---\")\n",
        "\n",
        "    collected_content = []\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Iterate through the plan generated by the previous node\n",
        "    for query in state['plan']:\n",
        "        print(f\"Searching for: {query}\")\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=2, # Keep low for speed in demo\n",
        "            sort_by=arxiv.SortCriterion.Relevance\n",
        "        )\n",
        "\n",
        "        for r in client.results(search):\n",
        "            paper_summary = f\"Title: {r.title}\\nAbstract: {r.summary[:500]}...\"\n",
        "            collected_content.append(paper_summary)\n",
        "\n",
        "    return {\"content\": collected_content}\n",
        "\n",
        "def writer_node(state: ResearchState):\n",
        "    print(\"--- ‚úçÔ∏è WRITER IS DRAFTING ---\")\n",
        "\n",
        "    # Combine all research into one context string\n",
        "    context_str = \"\\n\\n\".join(state['content'])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a Research Analyst. Synthesize the provided research summaries into a clear, structured report.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Task: {state['task']}\\n\\nResearch Materials:\\n{context_str}\"}\n",
        "    ]\n",
        "\n",
        "    draft = call_local_model(messages)\n",
        "\n",
        "    return {\n",
        "        \"draft\": draft,\n",
        "        \"revision_number\": state.get(\"revision_number\", 0) + 1\n",
        "    }\n",
        "\n",
        "def critic_node(state: ResearchState):\n",
        "    print(\"--- üßê CRITIC IS REVIEWING ---\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a strict Academic Reviewer. Check the draft. If it is high quality, reply with only the word 'APPROVE'. If it needs work, provide 1 sentence of feedback.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Draft: {state['draft']}\"}\n",
        "    ]\n",
        "\n",
        "    critique = call_local_model(messages)\n",
        "    return {\"critique\": critique}\n",
        "\n",
        "def should_continue(state: ResearchState):\n",
        "    critique = state.get('critique', '')\n",
        "    rev_num = state.get('revision_number', 0)\n",
        "    max_rev = state.get('max_revisions', 2)\n",
        "\n",
        "    if rev_num >= max_rev:\n",
        "        print(\"--- üõë MAX REVISIONS REACHED ---\")\n",
        "        return \"end\"\n",
        "\n",
        "    if \"APPROVE\" in critique.upper():\n",
        "        print(\"--- ‚úÖ DRAFT APPROVED ---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"--- üîÑ LOOPING BACK TO WRITER ---\")\n",
        "        return \"writer\" # In a complex app, this might go back to researcher\n",
        "\n",
        "# --- 4. BUILD THE GRAPH ---\n",
        "\n",
        "workflow = StateGraph(ResearchState)\n",
        "\n",
        "# Add Nodes\n",
        "workflow.add_node(\"planner\", planner_node)\n",
        "workflow.add_node(\"researcher\", researcher_node)\n",
        "workflow.add_node(\"writer\", writer_node)\n",
        "workflow.add_node(\"critic\", critic_node)\n",
        "\n",
        "# Set Entry Point\n",
        "workflow.set_entry_point(\"planner\")\n",
        "\n",
        "# Define Edges\n",
        "workflow.add_edge(\"planner\", \"researcher\")\n",
        "workflow.add_edge(\"researcher\", \"writer\")\n",
        "workflow.add_edge(\"writer\", \"critic\")\n",
        "\n",
        "# Conditional Edge (The Logic Loop)\n",
        "workflow.add_conditional_edges(\n",
        "    \"critic\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"writer\": \"writer\",  # If rejected, go back to writing (or researching)\n",
        "        \"end\": END           # If approved, finish\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"Graph compiled! Ready to run.\")"
      ],
      "metadata": {
        "id": "oPcU5qeeUPx5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6753893a-244c-4d8b-9efd-e81d1ec51bb9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph compiled! Ready to run.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the state\n",
        "initial_state = {\n",
        "    \"task\": \"Tell me about the newest and biggest changes in the SWE agents realm as of today (Jan 21, 2026)\",\n",
        "    \"max_revisions\": 2,\n",
        "    \"revision_number\": 0,\n",
        "    \"content\": [],\n",
        "    \"plan\": [],\n",
        "    \"draft\": \"\",\n",
        "    \"critique\": \"\"\n",
        "}\n",
        "\n",
        "# Variable to store the latest draft\n",
        "final_draft = \"\"\n",
        "\n",
        "print(\"Starting the Research Agent...\")\n",
        "\n",
        "# Run the graph\n",
        "for output in app.stream(initial_state):\n",
        "    for node_name, node_content in output.items():\n",
        "        print(f\"--- Finished running: {node_name} ---\")\n",
        "\n",
        "        # 1. Check if there are messages (common in 'planner' nodes)\n",
        "        if 'messages' in node_content:\n",
        "            # Get the last message's content and print it directly\n",
        "            last_msg = node_content['messages'][-1]\n",
        "            # Handle if it's an object or a dict\n",
        "            content = last_msg.content if hasattr(last_msg, 'content') else last_msg.get('content')\n",
        "            print(f\"FULL OUTPUT: {content}\\n\")\n",
        "\n",
        "        # 2. Check if there is a draft\n",
        "        if 'draft' in node_content:\n",
        "            final_draft = node_content['draft']\n",
        "            print(f\"Draft updated (Length: {len(final_draft)} chars)\")\n",
        "            # Optional: Print the first 500 chars to verify no dots\n",
        "            print(f\"Preview: {final_draft[:500]}...\\n\")\n",
        "\n",
        "# Print Final Result\n",
        "print(\"\\n\\n=== FINAL RESEARCH REPORT ===\")\n",
        "if final_draft:\n",
        "    print(final_draft)\n",
        "else:\n",
        "    print(\"No draft was generated (Did the loop crash or max out?)\")"
      ],
      "metadata": {
        "id": "dUZFseE6VBz8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10ec5da-cc0d-4618-bd3f-f1d5963996e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting the Research Agent...\n",
            "--- üß† PLANNER IS THINKING ---\n",
            "--- Finished running: planner ---\n",
            "--- üïµÔ∏è RESEARCHER IS SEARCHING ARXIV ---\n",
            "Searching for: SWE agents updates 2026\n",
            "Searching for: latest changes in SWE agents\n",
            "Searching for: newest SWE agents developments\n",
            "--- Finished running: researcher ---\n",
            "--- ‚úçÔ∏è WRITER IS DRAFTING ---\n",
            "--- Finished running: writer ---\n",
            "Draft updated (Length: 1719 chars)\n",
            "Preview: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The SWE agents realm has witnessed significant advancements as of January 21, 2026, with the introduction of two groundbreaking developments: SWE-Gym and SWE-rebench. These innovations aim to address the critical challenges faced in training and evaluating LLM-based software engineering agents.\n",
            "\n",
            "1. SWE-Gym: This is the first environment specifically designed for training real-world software engineering (SWE) agents. It contains 2,438 real-world Python task instances, each comp...\n",
            "\n",
            "--- üßê CRITIC IS REVIEWING ---\n",
            "--- üîÑ LOOPING BACK TO WRITER ---\n",
            "--- Finished running: critic ---\n",
            "--- ‚úçÔ∏è WRITER IS DRAFTING ---\n",
            "--- Finished running: writer ---\n",
            "Draft updated (Length: 741 chars)\n",
            "Preview: <think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The SWE agents realm has seen significant advancements as of today (Jan 21, 2026), with the introduction of SWE-Gym and SWE-rebench. SWE-Gym is the first environment for training real-world software engineering (SWE) agents, containing 2,438 real-world Python task instances. This environment has enabled training of language model based SWE agents, achieving up to 19% absolute gains in resolve rate on the popular SWE-Bench Verified and Lite test sets. Additionally, SWE-rebench ...\n",
            "\n",
            "--- üßê CRITIC IS REVIEWING ---\n",
            "--- üõë MAX REVISIONS REACHED ---\n",
            "--- Finished running: critic ---\n",
            "\n",
            "\n",
            "=== FINAL RESEARCH REPORT ===\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "The SWE agents realm has seen significant advancements as of today (Jan 21, 2026), with the introduction of SWE-Gym and SWE-rebench. SWE-Gym is the first environment for training real-world software engineering (SWE) agents, containing 2,438 real-world Python task instances. This environment has enabled training of language model based SWE agents, achieving up to 19% absolute gains in resolve rate on the popular SWE-Bench Verified and Lite test sets. Additionally, SWE-rebench has been introduced as an automated pipeline for task collection and decontaminated evaluation of SWE agents. This pipeline addresses the critical challenges of scarce high-quality training data and the need for real-world SWE scenario data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQGcsOixfbQt"
      },
      "execution_count": 5,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}