{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gqcpm/scholar_stream/blob/main/research_agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvQACbeaEjPu"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install langgraph langchain_core arxiv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo4aEF-WBXEh"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 1. Load the BASE model (The big 14B one)\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Qwen3-14B\", # Changed from Qwen3-14B to Qwen3-8B\n",
        "    max_seq_length = 1024,   # Context length - can be longer, but uses more memory\n",
        "    load_in_4bit = True,     # 4bit uses much less memory\n",
        "    full_finetuning = False, # We have full finetuning now!\n",
        "    # token = \"hf_...\",      # use one if using gated models\n",
        ")\n",
        "\n",
        "# 2. Load your ADAPTERS on top (The files you just saved)\n",
        "model.load_adapter(\"/content/drive/MyDrive/ai_models/lora_adapters\")\n",
        "\n",
        "# 3. Enable Inference Speedup\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"Model loaded successfully from Drive!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wB5uT4tMW-Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import arxiv\n",
        "from typing import TypedDict, List, Annotated\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import SystemMessage, HumanMessage\n",
        "\n",
        "# --- 1. DEFINE THE STATE ---\n",
        "# This dictionary tracks the data as it moves between agents\n",
        "class ResearchState(TypedDict):\n",
        "    task: str               # The user's original question\n",
        "    plan: List[str]         # The list of steps to research\n",
        "    content: List[str]      # The raw data gathered from ArXiv\n",
        "    draft: str              # The current written report\n",
        "    critique: str           # Feedback from the critic\n",
        "    revision_number: int    # To track iterations\n",
        "    max_revisions: int      # Limit to stop infinite loops\n",
        "\n",
        "# --- 2. HELPER: CONNECT UNSLOTH MODEL ---\n",
        "# This function wraps your loaded 'model' and 'tokenizer' to work like a chat bot\n",
        "def call_local_model(messages, max_tokens=1024):\n",
        "    \"\"\"\n",
        "    Formats messages for Qwen/Unsloth and generates a response.\n",
        "    \"\"\"\n",
        "    # Apply the specific chat template for your model (Qwen handles this well)\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Create inputs\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate\n",
        "    generated_ids = model.generate(\n",
        "        **model_inputs,\n",
        "        max_new_tokens=max_tokens,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    # Decode and strip the prompt (so we only get the new response)\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response_text\n",
        "\n",
        "# --- 3. DEFINE THE NODES (AGENTS) ---\n",
        "\n",
        "def planner_node(state: ResearchState):\n",
        "    print(\"--- üß† PLANNER IS THINKING ---\")\n",
        "\n",
        "    # Construct the prompt\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a Research Planner. Return a Python list of 3 short, specific search queries related to the user's task. Example format: ['query 1', 'query 2', 'query 3']. Do not explain, just return the list.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Task: {state['task']}\"}\n",
        "    ]\n",
        "\n",
        "    # Get response from your local model\n",
        "    response = call_local_model(messages)\n",
        "\n",
        "    # Simple parsing to ensure we get a list (Basic robustness)\n",
        "    # If the model chats too much, we try to extract the list part\n",
        "    try:\n",
        "        # Try to find the bracketed list in the text\n",
        "        import ast\n",
        "        start = response.find('[')\n",
        "        end = response.rfind(']') + 1\n",
        "        plan = ast.literal_eval(response[start:end])\n",
        "    except:\n",
        "        # Fallback if model fails to output strict list\n",
        "        plan = [f\"{state['task']} generic analysis\", f\"{state['task']} method comparison\"]\n",
        "\n",
        "    return {\"plan\": plan}\n",
        "\n",
        "def researcher_node(state: ResearchState):\n",
        "    print(\"--- üïµÔ∏è RESEARCHER IS SEARCHING ARXIV ---\")\n",
        "\n",
        "    collected_content = []\n",
        "    client = arxiv.Client()\n",
        "\n",
        "    # Iterate through the plan generated by the previous node\n",
        "    for query in state['plan']:\n",
        "        print(f\"Searching for: {query}\")\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=2, # Keep low for speed in demo\n",
        "            sort_by=arxiv.SortCriterion.Relevance\n",
        "        )\n",
        "\n",
        "        for r in client.results(search):\n",
        "            paper_summary = f\"Title: {r.title}\\nAbstract: {r.summary[:500]}...\"\n",
        "            collected_content.append(paper_summary)\n",
        "\n",
        "    return {\"content\": collected_content}\n",
        "\n",
        "def writer_node(state: ResearchState):\n",
        "    print(\"--- ‚úçÔ∏è WRITER IS DRAFTING ---\")\n",
        "\n",
        "    # Combine all research into one context string\n",
        "    context_str = \"\\n\\n\".join(state['content'])\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a Research Analyst. Synthesize the provided research summaries into a clear, structured report.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Task: {state['task']}\\n\\nResearch Materials:\\n{context_str}\"}\n",
        "    ]\n",
        "\n",
        "    draft = call_local_model(messages)\n",
        "\n",
        "    return {\n",
        "        \"draft\": draft,\n",
        "        \"revision_number\": state.get(\"revision_number\", 0) + 1\n",
        "    }\n",
        "\n",
        "def critic_node(state: ResearchState):\n",
        "    print(\"--- üßê CRITIC IS REVIEWING ---\")\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a strict Academic Reviewer. Check the draft. If it is high quality, reply with only the word 'APPROVE'. If it needs work, provide 1 sentence of feedback.\"},\n",
        "        {\"role\": \"user\", \"content\": f\"Draft: {state['draft']}\"}\n",
        "    ]\n",
        "\n",
        "    critique = call_local_model(messages)\n",
        "    return {\"critique\": critique}\n",
        "\n",
        "def should_continue(state: ResearchState):\n",
        "    critique = state.get('critique', '')\n",
        "    rev_num = state.get('revision_number', 0)\n",
        "    max_rev = state.get('max_revisions', 2)\n",
        "\n",
        "    if rev_num >= max_rev:\n",
        "        print(\"--- üõë MAX REVISIONS REACHED ---\")\n",
        "        return \"end\"\n",
        "\n",
        "    if \"APPROVE\" in critique.upper():\n",
        "        print(\"--- ‚úÖ DRAFT APPROVED ---\")\n",
        "        return \"end\"\n",
        "    else:\n",
        "        print(\"--- üîÑ LOOPING BACK TO WRITER ---\")\n",
        "        return \"writer\" # In a complex app, this might go back to researcher\n",
        "\n",
        "# --- 4. BUILD THE GRAPH ---\n",
        "\n",
        "workflow = StateGraph(ResearchState)\n",
        "\n",
        "# Add Nodes\n",
        "workflow.add_node(\"planner\", planner_node)\n",
        "workflow.add_node(\"researcher\", researcher_node)\n",
        "workflow.add_node(\"writer\", writer_node)\n",
        "workflow.add_node(\"critic\", critic_node)\n",
        "\n",
        "# Set Entry Point\n",
        "workflow.set_entry_point(\"planner\")\n",
        "\n",
        "# Define Edges\n",
        "workflow.add_edge(\"planner\", \"researcher\")\n",
        "workflow.add_edge(\"researcher\", \"writer\")\n",
        "workflow.add_edge(\"writer\", \"critic\")\n",
        "\n",
        "# Conditional Edge (The Logic Loop)\n",
        "workflow.add_conditional_edges(\n",
        "    \"critic\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"writer\": \"writer\",  # If rejected, go back to writing (or researching)\n",
        "        \"end\": END           # If approved, finish\n",
        "    }\n",
        ")\n",
        "\n",
        "# Compile\n",
        "app = workflow.compile()\n",
        "\n",
        "print(\"Graph compiled! Ready to run.\")"
      ],
      "metadata": {
        "id": "oPcU5qeeUPx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the state\n",
        "initial_state = {\n",
        "    \"task\": \"Compare the performance of Mamba vs Transformers in 2024\",\n",
        "    \"max_revisions\": 2,\n",
        "    \"revision_number\": 0,\n",
        "    \"content\": [],\n",
        "    \"plan\": [],\n",
        "    \"draft\": \"\",\n",
        "    \"critique\": \"\"\n",
        "}\n",
        "\n",
        "# Variable to store the latest draft\n",
        "final_draft = \"\"\n",
        "\n",
        "print(\"Starting the Research Agent...\")\n",
        "\n",
        "# Run the graph\n",
        "for output in app.stream(initial_state):\n",
        "    # output looks like: {'node_name': {'key': 'value'}}\n",
        "\n",
        "    for node_name, node_content in output.items():\n",
        "        print(f\"--- Finished running: {node_name} ---\")\n",
        "\n",
        "        # If this node produced a draft, save it!\n",
        "        if 'draft' in node_content:\n",
        "            final_draft = node_content['draft']\n",
        "            print(f\"Draft updated (Length: {len(final_draft)} chars)\")\n",
        "\n",
        "# Print Final Result\n",
        "print(\"\\n\\n=== FINAL RESEARCH REPORT ===\")\n",
        "if final_draft:\n",
        "    print(final_draft)\n",
        "else:\n",
        "    print(\"No draft was generated (Did the loop crash or max out?)\")"
      ],
      "metadata": {
        "id": "dUZFseE6VBz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 1. Mount Drive (if not already) to access the file\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Define a function to clean the specific 'widgets' error\n",
        "def clean_notebook_metadata(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        notebook = json.load(f)\n",
        "\n",
        "    # Check and delete the 'widgets' key from metadata if it exists\n",
        "    if 'metadata' in notebook and 'widgets' in notebook['metadata']:\n",
        "        del notebook['metadata']['widgets']\n",
        "        print(f\"‚úÖ Cleaned widgets metadata from: {file_path}\")\n",
        "\n",
        "        # Overwrite the file with the clean version\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(notebook, f, indent=1)\n",
        "    else:\n",
        "        print(\"NOTEBOOK IS ALREADY CLEAN (No widget metadata found).\")\n",
        "\n",
        "# 3. Apply it to your current notebook\n",
        "# NOTE: Replace 'Your_Notebook_Name.ipynb' with your actual file name in Drive\n",
        "# You can find the path by clicking the Folder icon on the left -> drive -> MyDrive -> Colab Notebooks\n",
        "notebook_path = \"/content/drive/MyDrive/Colab Notebooks/research_agent.ipynb\"\n",
        "\n",
        "if os.path.exists(notebook_path):\n",
        "    clean_notebook_metadata(notebook_path)\n",
        "    print(\"üëâ Now go to File -> Save a copy in GitHub\")\n",
        "else:\n",
        "    print(f\"‚ùå File not found at {notebook_path}. Please check the path.\")"
      ],
      "metadata": {
        "id": "aEo_Nxb5aM7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zQGcsOixfbQt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMtu3a2G3flmYeFOAJNDH1x",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}